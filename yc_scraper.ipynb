{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3tcUdCBpiVBT4Oa7HVhH8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jqlCWLCK3jrN","executionInfo":{"status":"ok","timestamp":1768668805851,"user_tz":-330,"elapsed":39716,"user":{"displayName":"Imasha Dilhari","userId":"04501571705707404872"}},"outputId":"53f43456-5080-401a-e3c2-6df152198013"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting playwright\n","  Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n","Collecting pyee<14,>=13 (from playwright)\n","  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n","Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.3.0)\n","Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Downloading playwright-1.57.0-py3-none-manylinux1_x86_64.whl (46.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyee-13.0.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyee, playwright\n","Successfully installed playwright-1.57.0 pyee-13.0.0\n","Downloading Chromium 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-linux.zip\u001b[22m\n","(node:815) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n","(Use `node --trace-deprecation ...` to show where the warning was created)\n","\u001b[1G164.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 0% 18.1s\u001b[0K\u001b[1G164.7 MiB [] 0% 11.9s\u001b[0K\u001b[1G164.7 MiB [] 0% 6.2s\u001b[0K\u001b[1G164.7 MiB [] 1% 3.7s\u001b[0K\u001b[1G164.7 MiB [] 2% 2.6s\u001b[0K\u001b[1G164.7 MiB [] 4% 2.3s\u001b[0K\u001b[1G164.7 MiB [] 4% 2.4s\u001b[0K\u001b[1G164.7 MiB [] 5% 2.0s\u001b[0K\u001b[1G164.7 MiB [] 7% 1.8s\u001b[0K\u001b[1G164.7 MiB [] 8% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 9% 1.7s\u001b[0K\u001b[1G164.7 MiB [] 11% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 12% 1.5s\u001b[0K\u001b[1G164.7 MiB [] 13% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 15% 1.4s\u001b[0K\u001b[1G164.7 MiB [] 17% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 18% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 19% 1.3s\u001b[0K\u001b[1G164.7 MiB [] 21% 1.2s\u001b[0K\u001b[1G164.7 MiB [] 23% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 25% 1.1s\u001b[0K\u001b[1G164.7 MiB [] 27% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 28% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 30% 1.0s\u001b[0K\u001b[1G164.7 MiB [] 32% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 34% 0.9s\u001b[0K\u001b[1G164.7 MiB [] 35% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 37% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 39% 0.8s\u001b[0K\u001b[1G164.7 MiB [] 40% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 42% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 44% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 45% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 46% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 48% 0.7s\u001b[0K\u001b[1G164.7 MiB [] 49% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 51% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 53% 0.6s\u001b[0K\u001b[1G164.7 MiB [] 55% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 56% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 57% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 58% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 60% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 62% 0.5s\u001b[0K\u001b[1G164.7 MiB [] 63% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 64% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 66% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 68% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 69% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 70% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 71% 0.4s\u001b[0K\u001b[1G164.7 MiB [] 72% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 73% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 74% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 76% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 77% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 79% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 80% 0.3s\u001b[0K\u001b[1G164.7 MiB [] 81% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 82% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 83% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 84% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 85% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 86% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 88% 0.2s\u001b[0K\u001b[1G164.7 MiB [] 89% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 90% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 94% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 95% 0.1s\u001b[0K\u001b[1G164.7 MiB [] 96% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 98% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 99% 0.0s\u001b[0K\u001b[1G164.7 MiB [] 100% 0.0s\u001b[0K\n","Chromium 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium-1200\n","Downloading Chromium Headless Shell 143.0.7499.4 (playwright build v1200)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-headless-shell-linux.zip\u001b[22m\n","(node:854) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n","(Use `node --trace-deprecation ...` to show where the warning was created)\n","\u001b[1G109.7 MiB [] 0% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 0% 14.5s\u001b[0K\u001b[1G109.7 MiB [] 0% 6.4s\u001b[0K\u001b[1G109.7 MiB [] 1% 2.7s\u001b[0K\u001b[1G109.7 MiB [] 3% 1.7s\u001b[0K\u001b[1G109.7 MiB [] 5% 1.3s\u001b[0K\u001b[1G109.7 MiB [] 6% 1.5s\u001b[0K\u001b[1G109.7 MiB [] 8% 1.2s\u001b[0K\u001b[1G109.7 MiB [] 10% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 11% 1.1s\u001b[0K\u001b[1G109.7 MiB [] 14% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 15% 1.0s\u001b[0K\u001b[1G109.7 MiB [] 17% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 19% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 20% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 22% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 23% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 24% 0.9s\u001b[0K\u001b[1G109.7 MiB [] 27% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 29% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 30% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 32% 0.8s\u001b[0K\u001b[1G109.7 MiB [] 34% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 37% 0.7s\u001b[0K\u001b[1G109.7 MiB [] 39% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 41% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 42% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 44% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 45% 0.6s\u001b[0K\u001b[1G109.7 MiB [] 46% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 48% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 50% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 51% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 53% 0.5s\u001b[0K\u001b[1G109.7 MiB [] 55% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 58% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 60% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 62% 0.4s\u001b[0K\u001b[1G109.7 MiB [] 65% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 67% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 68% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 69% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 70% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 72% 0.3s\u001b[0K\u001b[1G109.7 MiB [] 74% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 77% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 79% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 82% 0.2s\u001b[0K\u001b[1G109.7 MiB [] 84% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 86% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 89% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 91% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 93% 0.1s\u001b[0K\u001b[1G109.7 MiB [] 94% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 95% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 97% 0.0s\u001b[0K\u001b[1G109.7 MiB [] 100% 0.0s\u001b[0K\n","Chromium Headless Shell 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1200\n","Downloading Firefox 144.0.2 (playwright build v1497)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1497/firefox-ubuntu-22.04.zip\u001b[22m\n","(node:877) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n","(Use `node --trace-deprecation ...` to show where the warning was created)\n","\u001b[1G98.4 MiB [] 0% 0.0s\u001b[0K\u001b[1G98.4 MiB [] 0% 14.3s\u001b[0K\u001b[1G98.4 MiB [] 0% 8.6s\u001b[0K\u001b[1G98.4 MiB [] 1% 4.3s\u001b[0K\u001b[1G98.4 MiB [] 2% 2.9s\u001b[0K\u001b[1G98.4 MiB [] 4% 1.8s\u001b[0K\u001b[1G98.4 MiB [] 6% 1.4s\u001b[0K\u001b[1G98.4 MiB [] 7% 1.5s\u001b[0K\u001b[1G98.4 MiB [] 9% 1.2s\u001b[0K\u001b[1G98.4 MiB [] 12% 1.0s\u001b[0K\u001b[1G98.4 MiB [] 15% 0.9s\u001b[0K\u001b[1G98.4 MiB [] 18% 0.8s\u001b[0K\u001b[1G98.4 MiB [] 20% 0.7s\u001b[0K\u001b[1G98.4 MiB [] 22% 0.7s\u001b[0K\u001b[1G98.4 MiB [] 25% 0.7s\u001b[0K\u001b[1G98.4 MiB [] 28% 0.6s\u001b[0K\u001b[1G98.4 MiB [] 32% 0.6s\u001b[0K\u001b[1G98.4 MiB [] 34% 0.5s\u001b[0K\u001b[1G98.4 MiB [] 37% 0.5s\u001b[0K\u001b[1G98.4 MiB [] 40% 0.5s\u001b[0K\u001b[1G98.4 MiB [] 43% 0.4s\u001b[0K\u001b[1G98.4 MiB [] 44% 0.4s\u001b[0K\u001b[1G98.4 MiB [] 48% 0.4s\u001b[0K\u001b[1G98.4 MiB [] 49% 0.4s\u001b[0K\u001b[1G98.4 MiB [] 52% 0.3s\u001b[0K\u001b[1G98.4 MiB [] 56% 0.3s\u001b[0K\u001b[1G98.4 MiB [] 59% 0.3s\u001b[0K\u001b[1G98.4 MiB [] 60% 0.3s\u001b[0K\u001b[1G98.4 MiB [] 63% 0.3s\u001b[0K\u001b[1G98.4 MiB [] 65% 0.3s\u001b[0K\u001b[1G98.4 MiB [] 68% 0.2s\u001b[0K\u001b[1G98.4 MiB [] 72% 0.2s\u001b[0K\u001b[1G98.4 MiB [] 76% 0.2s\u001b[0K\u001b[1G98.4 MiB [] 79% 0.1s\u001b[0K\u001b[1G98.4 MiB [] 81% 0.1s\u001b[0K\u001b[1G98.4 MiB [] 85% 0.1s\u001b[0K\u001b[1G98.4 MiB [] 87% 0.1s\u001b[0K\u001b[1G98.4 MiB [] 89% 0.1s\u001b[0K\u001b[1G98.4 MiB [] 91% 0.1s\u001b[0K\u001b[1G98.4 MiB [] 93% 0.0s\u001b[0K\u001b[1G98.4 MiB [] 96% 0.0s\u001b[0K\u001b[1G98.4 MiB [] 98% 0.0s\u001b[0K\u001b[1G98.4 MiB [] 100% 0.0s\u001b[0K\n","Firefox 144.0.2 (playwright build v1497) downloaded to /root/.cache/ms-playwright/firefox-1497\n","Downloading Webkit 26.0 (playwright build v2227)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2227/webkit-ubuntu-22.04.zip\u001b[22m\n","(node:916) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n","(Use `node --trace-deprecation ...` to show where the warning was created)\n","\u001b[1G96.1 MiB [] 0% 0.0s\u001b[0K\u001b[1G96.1 MiB [] 0% 14.7s\u001b[0K\u001b[1G96.1 MiB [] 0% 9.9s\u001b[0K\u001b[1G96.1 MiB [] 1% 4.2s\u001b[0K\u001b[1G96.1 MiB [] 2% 3.4s\u001b[0K\u001b[1G96.1 MiB [] 2% 2.9s\u001b[0K\u001b[1G96.1 MiB [] 3% 2.6s\u001b[0K\u001b[1G96.1 MiB [] 4% 2.3s\u001b[0K\u001b[1G96.1 MiB [] 6% 2.0s\u001b[0K\u001b[1G96.1 MiB [] 7% 2.0s\u001b[0K\u001b[1G96.1 MiB [] 9% 1.8s\u001b[0K\u001b[1G96.1 MiB [] 11% 1.6s\u001b[0K\u001b[1G96.1 MiB [] 13% 1.4s\u001b[0K\u001b[1G96.1 MiB [] 14% 1.3s\u001b[0K\u001b[1G96.1 MiB [] 16% 1.2s\u001b[0K\u001b[1G96.1 MiB [] 18% 1.1s\u001b[0K\u001b[1G96.1 MiB [] 20% 1.1s\u001b[0K\u001b[1G96.1 MiB [] 22% 1.0s\u001b[0K\u001b[1G96.1 MiB [] 24% 1.0s\u001b[0K\u001b[1G96.1 MiB [] 28% 0.9s\u001b[0K\u001b[1G96.1 MiB [] 31% 0.8s\u001b[0K\u001b[1G96.1 MiB [] 32% 0.8s\u001b[0K\u001b[1G96.1 MiB [] 34% 0.7s\u001b[0K\u001b[1G96.1 MiB [] 37% 0.7s\u001b[0K\u001b[1G96.1 MiB [] 40% 0.6s\u001b[0K\u001b[1G96.1 MiB [] 44% 0.6s\u001b[0K\u001b[1G96.1 MiB [] 47% 0.5s\u001b[0K\u001b[1G96.1 MiB [] 49% 0.5s\u001b[0K\u001b[1G96.1 MiB [] 51% 0.5s\u001b[0K\u001b[1G96.1 MiB [] 54% 0.4s\u001b[0K\u001b[1G96.1 MiB [] 56% 0.4s\u001b[0K\u001b[1G96.1 MiB [] 59% 0.4s\u001b[0K\u001b[1G96.1 MiB [] 61% 0.4s\u001b[0K\u001b[1G96.1 MiB [] 64% 0.3s\u001b[0K\u001b[1G96.1 MiB [] 68% 0.3s\u001b[0K\u001b[1G96.1 MiB [] 71% 0.3s\u001b[0K\u001b[1G96.1 MiB [] 73% 0.2s\u001b[0K\u001b[1G96.1 MiB [] 75% 0.2s\u001b[0K\u001b[1G96.1 MiB [] 79% 0.2s\u001b[0K\u001b[1G96.1 MiB [] 83% 0.1s\u001b[0K\u001b[1G96.1 MiB [] 86% 0.1s\u001b[0K\u001b[1G96.1 MiB [] 89% 0.1s\u001b[0K\u001b[1G96.1 MiB [] 92% 0.1s\u001b[0K\u001b[1G96.1 MiB [] 96% 0.0s\u001b[0K\u001b[1G96.1 MiB [] 100% 0.0s\u001b[0K\n","Webkit 26.0 (playwright build v2227) downloaded to /root/.cache/ms-playwright/webkit-2227\n","Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001b[22m\n","(node:939) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n","(Use `node --trace-deprecation ...` to show where the warning was created)\n","\u001b[1G2.3 MiB [] 0% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 5% 0.3s\u001b[0K\u001b[1G2.3 MiB [] 23% 0.1s\u001b[0K\u001b[1G2.3 MiB [] 53% 0.0s\u001b[0K\u001b[1G2.3 MiB [] 100% 0.0s\u001b[0K\n","FFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\n","Playwright Host validation warning: \n","╔══════════════════════════════════════════════════════╗\n","║ Host system is missing dependencies to run browsers. ║\n","║ Please install them with the following command:      ║\n","║                                                      ║\n","║     playwright install-deps                          ║\n","║                                                      ║\n","║ Alternatively, use apt:                              ║\n","║     apt-get install libxcomposite1\\                  ║\n","║         libgtk-3-0\\                                  ║\n","║         libatk1.0-0                                  ║\n","║                                                      ║\n","║ <3 Playwright Team                                   ║\n","╚══════════════════════════════════════════════════════╝\n","    at validateDependenciesLinux (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n","\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:103:5)\u001b[39m\n","    at async Registry._validateHostRequirements (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:990:14)\n","    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1112:7)\n","    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1101:7)\n","    at async r.<anonymous> (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/cli/program.js:176:7)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  at-spi2-core gsettings-desktop-schemas libatk-bridge2.0-0 libatk1.0-data\n","  libatspi2.0-0 libgtk-3-bin libgtk-3-common librsvg2-common libxtst6\n","  session-migration\n","Suggested packages:\n","  gvfs\n","The following NEW packages will be installed:\n","  at-spi2-core gsettings-desktop-schemas libatk-bridge2.0-0 libatk1.0-0\n","  libatk1.0-data libatspi2.0-0 libgtk-3-0 libgtk-3-bin libgtk-3-common\n","  librsvg2-common libxcomposite1 libxtst6 session-migration\n","0 upgraded, 13 newly installed, 0 to remove and 1 not upgraded.\n","Need to get 3,697 kB of archives.\n","After this operation, 12.9 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n","Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n","Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n","Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-common all 3.24.33-1ubuntu2.2 [239 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-0 amd64 3.24.33-1ubuntu2.2 [3,053 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk-3-bin amd64 3.24.33-1ubuntu2.2 [69.6 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n","Fetched 3,697 kB in 2s (2,361 kB/s)\n","Selecting previously unselected package libatspi2.0-0:amd64.\n","(Reading database ... 117528 files and directories currently installed.)\n","Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n","Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n","Selecting previously unselected package libxtst6:amd64.\n","Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n","Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n","Selecting previously unselected package session-migration.\n","Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n","Unpacking session-migration (0.3.6) ...\n","Selecting previously unselected package gsettings-desktop-schemas.\n","Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n","Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n","Selecting previously unselected package at-spi2-core.\n","Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n","Unpacking at-spi2-core (2.44.0-3) ...\n","Selecting previously unselected package libatk1.0-data.\n","Preparing to unpack .../05-libatk1.0-data_2.36.0-3build1_all.deb ...\n","Unpacking libatk1.0-data (2.36.0-3build1) ...\n","Selecting previously unselected package libatk1.0-0:amd64.\n","Preparing to unpack .../06-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n","Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n","Selecting previously unselected package libatk-bridge2.0-0:amd64.\n","Preparing to unpack .../07-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n","Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n","Selecting previously unselected package libxcomposite1:amd64.\n","Preparing to unpack .../08-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n","Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n","Selecting previously unselected package libgtk-3-common.\n","Preparing to unpack .../09-libgtk-3-common_3.24.33-1ubuntu2.2_all.deb ...\n","Unpacking libgtk-3-common (3.24.33-1ubuntu2.2) ...\n","Selecting previously unselected package libgtk-3-0:amd64.\n","Preparing to unpack .../10-libgtk-3-0_3.24.33-1ubuntu2.2_amd64.deb ...\n","Unpacking libgtk-3-0:amd64 (3.24.33-1ubuntu2.2) ...\n","Selecting previously unselected package libgtk-3-bin.\n","Preparing to unpack .../11-libgtk-3-bin_3.24.33-1ubuntu2.2_amd64.deb ...\n","Unpacking libgtk-3-bin (3.24.33-1ubuntu2.2) ...\n","Selecting previously unselected package librsvg2-common:amd64.\n","Preparing to unpack .../12-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n","Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n","Setting up session-migration (0.3.6) ...\n","Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n","Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n","Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n","Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n","Setting up libatk1.0-data (2.36.0-3build1) ...\n","Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n","Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n","Setting up libgtk-3-common (3.24.33-1ubuntu2.2) ...\n","Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n","Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.4) ...\n","Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n","Setting up libgtk-3-0:amd64 (3.24.33-1ubuntu2.2) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","Setting up libgtk-3-bin (3.24.33-1ubuntu2.2) ...\n","Setting up at-spi2-core (2.44.0-3) ...\n"]}],"source":["import sys\n","!{sys.executable} -m pip install playwright pandas\n","!playwright install\n","!apt-get install libxcomposite1 libgtk-3-0 libatk1.0-0 -y"]},{"cell_type":"code","source":["import asyncio\n","from playwright.async_api import async_playwright\n","import pandas as pd\n","import time\n","\n","# ~500 companies\n","TARGET_COUNT = 500\n","YC_URL = \"https://www.ycombinator.com/companies\"\n","\n","async def scrape_yc():\n","    async with async_playwright() as p:\n","        browser = await p.chromium.launch(headless=True) # Changed to headless=True\n","        page = await browser.new_page()\n","\n","        print(f\"Loading {YC_URL}...\")\n","        await page.goto(YC_URL)\n","\n","#-------PHASE 1 TO 4 --------------------------------------\n","        # --- PHASE 1: SCROLL & LOAD COMPANIES ---\n","        companies_data = []\n","        unique_urls = set()\n","\n","        # Locator for company cards (generic selector to be robust against class name changes)\n","        # Looking for links that go to /companies/\n","        company_locator = page.locator('a[href^=\"/companies/\"]')\n","\n","        print(\"Scrolling to load companies...\")\n","        while len(unique_urls) < TARGET_COUNT:\n","            # Scroll to bottom\n","            await page.evaluate(\"window.scrollTo(0, document.body.scrollHeight)\")\n","            await page.wait_for_timeout(1000) # Wait for content to load\n","\n","            # Extract current list\n","            # We filter for links that look like company cards (usually contain the company name header)\n","            # Note: YC class names are hashed (e.g., _company_86jzd), so we use structure/attributes.\n","            count = await company_locator.count()\n","            print(f\"Loaded {count} companies...\")\n","\n","            if count >= TARGET_COUNT:\n","                break\n","\n","        # --- PHASE 2: PARSE MAIN LIST ---\n","        print(\"Parsing loaded list...\")\n","        # Get all company elements\n","        elements = await company_locator.all()\n","\n","        for el in elements[:TARGET_COUNT]:\n","            href = await el.get_attribute(\"href\")\n","            full_url = f\"https://www.ycombinator.com{href}\"\n","\n","            if full_url in unique_urls:\n","                continue\n","            unique_urls.add(full_url)\n","\n","            # Extract basic info from the card text\n","            # The card usually contains: Name, Location, Description, Batch\n","            text_content = await el.inner_text()\n","            lines = text_content.split('\\n')\n","\n","            # Simple heuristic parsing (structure varies, but usually Name is top)\n","            name = lines[0] if lines else \"N/A\"\n","            description = lines[-1] if len(lines) > 1 else \"N/A\"\n","\n","            # Batch is often clearly labeled or in a specific tag, but extraction from list text\n","            # can be messy. We can also grab it from the detail page to be safe.\n","\n","            companies_data.append({\n","                \"Company Name\": name,\n","                \"URL\": full_url,\n","                \"Short Description\": description,\n","                # Placeholders to be filled in Phase 3\n","                \"Batch\": \"Pending\",\n","                \"Founder Name(s)\": [],\n","                \"Founder LinkedIn URL(s)\": []\n","            })\n","\n","        print(f\"Collected {len(companies_data)} unique profiles. Starting detailed scrape...\")\n","\n","        # --- PHASE 3: DETAILED SCRAPING (Profile Visits) ---\n","        # For efficiency, we could use asyncio.gather, but let's loop for simplicity/rate-limiting\n","        for i, company in enumerate(companies_data):\n","            try:\n","                print(f\"[{i+1}/{len(companies_data)}] Scraping {company['Company Name']}...\")\n","                await page.goto(company['URL'])\n","                await page.wait_for_load_state('domcontentloaded')\n","\n","                # 1. Extract Batch (often in a pill header)\n","                # Look for text like \"W24\", \"S23\", \"Winter 2024\"\n","                batch_locator = page.locator('a[href*=\"batch=\"], span:has-text(\"Winter\"), span:has-text(\"Summer\"), span:has-text(\"Spring\"), span:has-text(\"Fall\")')\n","                if await batch_locator.count() > 0:\n","                     company[\"Batch\"] = await batch_locator.first.inner_text()\n","\n","                # 2. Extract Founders\n","                # Founders are usually in a section. We look for the \"Founders\" header\n","                # or containers with founder info.\n","                founders = []\n","                linkedins = []\n","\n","                # Strategy: Find the founder section, then find names and links inside it\n","                # Robust selector: Look for div cards inside a container that has \"Active Founders\" or just \"Founders\"\n","                founder_elements = page.locator('.space-y-5 > div') # Common layout for founder rows\n","\n","                # If specific layout fails, generic approach:\n","                # Find all links to linkedin.com on the page\n","                linkedin_els = page.locator('a[href*=\"linkedin.com\"]')\n","                count = await linkedin_els.count()\n","\n","                for j in range(count):\n","                    url = await linkedin_els.nth(j).get_attribute('href')\n","                    # Often the name is the text of the link or the parent container's text\n","                    # Here we take a simplified approach: Gather all unique LinkedIn URLs\n","                    if url and \"ycombinator\" not in url:\n","                        linkedins.append(url)\n","\n","                # Try to get names specifically (usually h3 or bold text near the photo)\n","                name_els = page.locator('h3') # Founders often have h3 headings\n","                name_count = await name_els.count()\n","                for k in range(name_count):\n","                    text = await name_els.nth(k).inner_text()\n","                    if text and text not in [\"Latest News\", \"Similar Companies\"]:\n","                        founders.append(text)\n","\n","                company[\"Founder Name(s)\"] = \", \".join(list(set(founders)))\n","                company[\"Founder LinkedIn URL(s)\"] = \", \".join(list(set(linkedins)))\n","\n","            except Exception as e:\n","                print(f\"Error scraping {company['Company Name']}: {e}\")\n","\n","        await browser.close()\n","\n","        # --- PHASE 4: SAVE TO CSV ---\n","        df = pd.DataFrame(companies_data)\n","        df.to_csv(\"yc_startups_500.csv\", index=False)\n","        print(\"Done! Data saved to yc_startups_500.csv\")\n","\n","# Run the async function\n","await scrape_yc()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d_Mh7JW14Zat","executionInfo":{"status":"ok","timestamp":1768669554033,"user_tz":-330,"elapsed":446063,"user":{"displayName":"Imasha Dilhari","userId":"04501571705707404872"}},"outputId":"b5f1abd4-5d86-4c6a-f103-63461b8d89b7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading https://www.ycombinator.com/companies...\n","Scrolling to load companies...\n","Loaded 40 companies...\n","Loaded 60 companies...\n","Loaded 80 companies...\n","Loaded 100 companies...\n","Loaded 120 companies...\n","Loaded 140 companies...\n","Loaded 160 companies...\n","Loaded 180 companies...\n","Loaded 200 companies...\n","Loaded 220 companies...\n","Loaded 240 companies...\n","Loaded 260 companies...\n","Loaded 280 companies...\n","Loaded 300 companies...\n","Loaded 320 companies...\n","Loaded 340 companies...\n","Loaded 360 companies...\n","Loaded 380 companies...\n","Loaded 400 companies...\n","Loaded 420 companies...\n","Loaded 440 companies...\n","Loaded 460 companies...\n","Loaded 480 companies...\n","Loaded 500 companies...\n","Parsing loaded list...\n","Collected 500 unique profiles. Starting detailed scrape...\n","[1/500] Scraping DoorDashSan Francisco, CA, USA...\n","[2/500] Scraping AirbnbSan Francisco, CA, USA...\n","[3/500] Scraping CoinbaseSan Francisco, CA, USA...\n","[4/500] Scraping OkloSanta Clara, CA, USA...\n","[5/500] Scraping GrowwBengaluru, KA, India...\n","[6/500] Scraping InstacartSan Francisco, CA, USA...\n","[7/500] Scraping MeeshoBengaluru, KA, India...\n","[8/500] Scraping Rigetti ComputingBerkeley, CA, USA...\n","[9/500] Scraping DropboxSan Francisco, CA, USA...\n","[10/500] Scraping GitLabSan Francisco, CA, USA...\n","[11/500] Scraping BillionToOneMenlo Park, CA, USA...\n","[12/500] Scraping MatterportSunnyvale, CA, USA...\n","[13/500] Scraping AmplitudeSan Francisco, CA, USA...\n","[14/500] Scraping PagerDutySan Francisco, CA, USA...\n","[15/500] Scraping Ginkgo BioworksBoston, MA, USA...\n","[16/500] Scraping WeaveLehi, UT, USA...\n","[17/500] Scraping Pardes BiosciencesSan Francisco, CA, USA...\n","[18/500] Scraping Embark TrucksSan Francisco, CA, USA...\n","[19/500] Scraping MomentusSanta Clara, CA, USA...\n","[20/500] Scraping Lucira HealthEmeryville, CA, USA...\n","[21/500] Scraping SegmentSan Francisco, CA, USA...\n","[22/500] Scraping AlgoliaSan Francisco, CA, USA...\n","[23/500] Scraping TruebillSilver Spring, MD, USA...\n","[24/500] Scraping TwitchSan Francisco, CA, USA...\n","[25/500] Scraping PlanGridSan Francisco, CA, USA...\n","[26/500] Scraping BellabeatSan Francisco, CA, USA...\n","[27/500] Scraping CruiseSan Francisco, CA, USA...\n","[28/500] Scraping BenchlingSan Francisco, CA, USA...\n","[29/500] Scraping CasetextSan Francisco, CA, USA...\n","[30/500] Scraping BirdAmsterdam, NH, Netherlands...\n","[31/500] Scraping BrexSan Francisco, CA, USA...\n","[32/500] Scraping The AthleticSan Francisco, CA, USA...\n","[33/500] Scraping CodecademyNew York, NY, USA...\n","[34/500] Scraping CheckrSan Francisco, CA, USA...\n","[35/500] Scraping LeverSan Francisco, CA, USA...\n","[36/500] Scraping ClipboardSan Francisco, CA, USA...\n","[37/500] Scraping HeapSan Francisco, CA, USA...\n","[38/500] Scraping SendwaveBoston, MA, USA...\n","[39/500] Scraping DeelSan Francisco, CA, USA...\n","[40/500] Scraping CleverSan Francisco, CA, USA...\n","[41/500] Scraping Caper...\n","[42/500] Scraping EquipmentShareColumbia, MO, USA...\n","[43/500] Scraping RedditSan Francisco, CA, USA...\n","[44/500] Scraping FivestarsSan Francisco, CA, USA...\n","[45/500] Scraping Machine ZonePalo Alto, CA, USA...\n","[46/500] Scraping FaireSan Francisco, CA, USA...\n","[47/500] Scraping FivetranOakland, CA, USA...\n","[48/500] Scraping OptimizelySan Francisco, CA, USA...\n","[49/500] Scraping FlexportSan Francisco, CA, USA...\n","[50/500] Scraping WePayRedwood City, CA, USA...\n","[51/500] Scraping WeeblySan Francisco, CA, USA...\n","[52/500] Scraping Flock SafetyAtlanta, GA, USA...\n","[53/500] Scraping SqreenSan Francisco, CA, USA...\n","[54/500] Scraping NURXNew York, NY, USA...\n","[55/500] Scraping Go1San Francisco, CA, USA...\n","[56/500] Scraping CoreOSSan Francisco, CA, USA...\n","[57/500] Scraping Bear Flag RoboticsNewark, CA, USA...\n","[58/500] Scraping GOAT GroupLos Angeles, CA, USA...\n","[59/500] Scraping HerokuSan Francisco, CA, USA...\n","[60/500] Scraping GrubMarketSan Francisco, CA, USA...\n","[61/500] Scraping HelloSignSan Francisco, CA, USA...\n","[62/500] Scraping ZenefitsSan Francisco, CA, USA...\n","[63/500] Scraping GustoSan Francisco, CA, USA...\n","[64/500] Scraping HoneyloveLos Angeles, CA, USA...\n","[65/500] Scraping Modern FertilitySan Francisco, CA, USA...\n","[66/500] Scraping CognitoPalo Alto, CA, USA...\n","[67/500] Scraping OpenInvestSan Francisco, CA, USA...\n","[68/500] Scraping MixpanelSan Francisco, CA, USA...\n","[69/500] Scraping PaystackLagos, LA, Nigeria...\n","[70/500] Scraping Moxion Power Co.Richmond, CA, USA...\n","[71/500] Scraping DrChronoCA, USA...\n","[72/500] Scraping OMGPopNew York, NY, USA...\n","[73/500] Scraping NewfrontSan Francisco, CA, USA...\n","[74/500] Scraping NorthKitchener, ON, Canada...\n","[75/500] Scraping Nowports...\n","[76/500] Scraping OdekoNew York, NY, USA...\n","[77/500] Scraping GitPrimeDurango, CO, USA...\n","[78/500] Scraping ProxySan Francisco, CA, USA...\n","[79/500] Scraping FutureAdvisorSan Francisco, CA, USA...\n","[80/500] Scraping PodiumLehi, UT, USA...\n","[81/500] Scraping RappiBogotá, Bogota, Colombia...\n","[82/500] Scraping RazorpayBengaluru, KA, India...\n","[83/500] Scraping RipplingSan Francisco, CA, USA...\n","[84/500] Scraping Scale AISan Francisco, CA, USA...\n","[85/500] Scraping ScentbirdNew York, NY, USA...\n","[86/500] Scraping ScribdSan Francisco, CA, USA...\n","[87/500] Scraping ShipBobChicago, IL, USA...\n","[88/500] Scraping SmartAssetNew York, NY, USA...\n","[89/500] Scraping StripeSan Francisco, CA, USA...\n","[90/500] Scraping WaveDakar, Dakar Region, Senegal...\n","[91/500] Scraping WebflowSan Francisco, CA, USA...\n","[92/500] Scraping WhatnotLos Angeles, CA, USA...\n","[93/500] Scraping ZapierMountain View, CA, USA...\n","[94/500] Scraping ZeptoMH, India...\n","[95/500] Scraping Focal SystemsNew York, NY, USA...\n","[96/500] Scraping MioAustin, TX, USA...\n","[97/500] Scraping DailySan Francisco, CA, USA...\n","[98/500] Scraping PetcubeSan Francisco, CA, USA...\n","[99/500] Scraping OutschoolSan Francisco, CA, USA...\n","[100/500] Scraping MasonSeattle, WA, USA...\n","[101/500] Scraping MagicBusLos Angeles, CA, USA...\n","[102/500] Scraping TovalaChicago, IL, USA...\n","[103/500] Scraping iSono HealthSan Francisco, CA, USA...\n","[104/500] Scraping GetAcceptSan Francisco, CA, USA...\n","[105/500] Scraping ChatfuelSan Francisco, CA, USA...\n","[106/500] Scraping YardbookSan Mateo, CA, USA...\n","[107/500] Scraping GoCardlessLondon, England, United Kingdom...\n","[108/500] Scraping Stealth WorkerMenlo Park, CA, USA...\n","[109/500] Scraping DeepgramSan Francisco, CA, USA...\n","[110/500] Scraping SOUNDBOKSCopenhagen, Denmark...\n","[111/500] Scraping LatticeSan Francisco, CA, USA...\n","[112/500] Scraping MuxSan Francisco, CA, USA...\n","[113/500] Scraping Human InterestSan Francisco, CA, USA...\n","[114/500] Scraping Flirtey (SkyDrop)Reno, NV, USA...\n","[115/500] Scraping Eight SleepNew York, NY, USA...\n","[116/500] Scraping InstaworkSan Francisco, CA, USA...\n","[117/500] Scraping Cofactor GenomicsSt. Louis, MO, USA...\n","[118/500] Scraping Click & GrowPalo Alto, CA, USA...\n","[119/500] Scraping BitmovinSan Francisco, CA, USA...\n","[120/500] Scraping TetraScienceBoston, MA, USA...\n","[121/500] Scraping The Ticket FairyLos Angeles, CA, USA...\n","[122/500] Scraping Font AwesomeBentonville, AR, USA...\n","[123/500] Scraping 80,000 HoursLondon, England, United Kingdom...\n","[124/500] Scraping XenditJakarta, Jakarta, Indonesia...\n","[125/500] Scraping Shred VideoSan Francisco, CA, USA...\n","[126/500] Scraping Shape (ShapeScale)San Francisco, CA, USA...\n","[127/500] Scraping RedCarpetUpGurugram, HR, India...\n","[128/500] Scraping Plate IQNew York, NY, USA...\n","[129/500] Scraping PickTraceLos Angeles, CA, USA...\n","[130/500] Scraping FountainSan Francisco, CA, USA...\n","[131/500] Scraping MicrohealthNew York, NY, USA...\n","[132/500] Scraping ZeplinSan Francisco, CA, USA...\n","[133/500] Scraping MarkhorSan Francisco, CA, USA...\n","[134/500] Scraping Circle Medical...\n","[135/500] Scraping GiveCampusWashington, DC, USA...\n","[136/500] Scraping Drip CapitalMH, India...\n","[137/500] Scraping ChaldalDhaka, Dhaka Division, Bangladesh...\n","[138/500] Scraping ClerkyPalo Alto, CA, USA...\n","[139/500] Scraping BodyportSan Francisco, CA, USA...\n","[140/500] Scraping Verge Genomics...\n","[141/500] Scraping Thrive AgritechNew York, NY, USA...\n","[142/500] Scraping TesorioSan Francisco, CA, USA...\n","[143/500] Scraping AgileMDSan Francisco, CA, USA...\n","[144/500] Scraping ReachRedwood City, CA, USA...\n","[145/500] Scraping SunFarmerKathmandu, Central Development Region, Nepal...\n","[146/500] Scraping Confident LIMSPalo Alto, CA, USA...\n","[147/500] Scraping Branch8Kowloon, Hong Kong...\n","[148/500] Scraping Heroic LabsLondon, England, United Kingdom...\n","[149/500] Scraping SnapMagicSan Francisco, CA, USA...\n","[150/500] Scraping Scope ARSan Francisco, CA, USA...\n","[151/500] Scraping PartnerStackToronto, ON, Canada...\n","[152/500] Scraping AssemblySouth San Francisco, CA, USA...\n","[153/500] Scraping GemnoteSan Leandro, CA, USA...\n","[154/500] Scraping New StoryAtlanta, GA, USA...\n","[155/500] Scraping IroncladSan Francisco, CA, USA...\n","[156/500] Scraping Leaders In TechSan Francisco, CA, USA...\n","[157/500] Scraping teaBOTToronto, ON, Canada...\n","[158/500] Scraping TeleportOakland, CA, USA...\n","[159/500] Scraping LuggSan Francisco, CA, USA...\n","[160/500] Scraping ShasqiSan Francisco, CA, USA...\n","[161/500] Scraping MashginPalo Alto, CA, USA...\n","[162/500] Scraping NimbleRxRedwood City, CA, USA...\n","[163/500] Scraping QuartzyHayward, CA, USA...\n","[164/500] Scraping ReplikaSan Francisco, CA, USA...\n","[165/500] Scraping Tara AISan Jose, CA, USA...\n","[166/500] Scraping FlipLos Angeles, CA, USA...\n","[167/500] Scraping X-ZellSingapore, Singapore...\n","[168/500] Scraping ZenflowSouth San Francisco, CA, USA...\n","[169/500] Scraping TabLondon, England, United Kingdom...\n","[170/500] Scraping StreakRemote...\n","[171/500] Scraping MeadowSan Francisco, CA, USA...\n","[172/500] Scraping Level FramesNY, USA...\n","[173/500] Scraping Industrial MicrobesAlameda, CA, USA...\n","[174/500] Scraping GiveffectNew York, NY, USA...\n","[175/500] Scraping eBrandvalueMountain View, CA, USA...\n","[176/500] Scraping QuantierraNew York, NY, USA...\n","[177/500] Scraping CleanlyNew York, NY, USA...\n","[178/500] Scraping ResolveNew York, NY, USA...\n","[179/500] Scraping MakrwatchNew York, NY, USA...\n","[180/500] Scraping TeamNoteHong Kong, Hong Kong...\n","[181/500] Scraping TempoSan Francisco, CA, USA...\n","[182/500] Scraping SIRUMPalo Alto, CA, USA...\n","[183/500] Scraping Sails Co.Austin, TX, USA...\n","[184/500] Scraping ReadMeSan Francisco, CA, USA...\n","[185/500] Scraping BuildScienceNiagara Falls, NY, USA...\n","[186/500] Scraping PlatziSan Francisco, CA, USA...\n","[187/500] Scraping LabdoorSan Francisco, CA, USA...\n","[188/500] Scraping GiveMeTapLondon, England, United Kingdom...\n","[189/500] Scraping BankjoyTroy, MI, USA...\n","[190/500] Scraping ZeitviewSanta Monica, CA, USA...\n","[191/500] Scraping The Human UtilityOakland, CA, USA...\n","[192/500] Scraping Democracy EarthMadrid, Community of Madrid, Spain...\n","[193/500] Scraping OneSignalSan Mateo, CA, USA...\n","[194/500] Scraping Numerion LabsSan Francisco, CA, USA...\n","[195/500] Scraping BrightMexico City, CDMX, Mexico...\n","[196/500] Scraping MagicWashington, DC, USA...\n","[197/500] Scraping MezmoSan Jose, CA, USA...\n","[198/500] Scraping Akido LabsLos Angeles, CA, USA...\n","[199/500] Scraping QventusSan Francisco, CA, USA...\n","[200/500] Scraping Bayes ImpactSan Francisco, CA, USA...\n","[201/500] Scraping Helion EnergyEverett, WA, USA...\n","[202/500] Scraping HackerRankMountain View, CA, USA...\n","[203/500] Scraping FrontSan Francisco, CA, USA...\n","[204/500] Scraping ApolloSan Francisco, CA, USA...\n","[205/500] Scraping RoomstormRedwood City, CA, USA...\n","[206/500] Scraping HiveKitchener, ON, Canada...\n","[207/500] Scraping TenjinSan Francisco, CA, USA...\n","[208/500] Scraping SpotAngelsSan Francisco, CA, USA...\n","[209/500] Scraping UserGemsAustria...\n","[210/500] Scraping SFOXLos Angeles, CA, USA...\n","[211/500] Scraping PicnicHealthSan Francisco, CA, USA...\n","[212/500] Scraping PermutiveLondon, England, United Kingdom...\n","[213/500] Scraping AlchemyWaterloo, ON, Canada...\n","[214/500] Scraping MTailorSan Francisco, CA, USA...\n","[215/500] Scraping MetricWireKitchener, ON, Canada...\n","[216/500] Scraping Protocol LabsPalo Alto, CA, USA...\n","[217/500] Scraping GreentoeNew York, NY, USA...\n","[218/500] Scraping HyperpadLondon, ON, Canada...\n","[219/500] Scraping FlaviarNew York, NY, USA...\n","[220/500] Scraping ClearBengaluru, KA, India...\n","[221/500] Scraping BillforwardSan Francisco, CA, USA...\n","[222/500] Scraping BackpackSan Francisco, CA, USA...\n","[223/500] Scraping Immunity ProjectSan Francisco, CA, USA...\n","[224/500] Scraping DyspatchVictoria, BC, Canada...\n","[225/500] Scraping OpenCurriculumMountain View, CA, USA...\n","[226/500] Scraping MixerBoxPalo Alto, CA, USA...\n","[227/500] Scraping Y Combinator...\n","[228/500] Scraping GBatteriesOttawa, ON, Canada...\n","[229/500] Scraping ZincSan Francisco, CA, USA...\n","[230/500] Scraping ZidishaSterling, VA, USA...\n","[231/500] Scraping TrueVaultSan Francisco, CA, USA...\n","[232/500] Scraping GuestyTel Aviv-Yafo, Tel Aviv District, Israel...\n","[233/500] Scraping SnapdocsSan Francisco, CA, USA...\n","[234/500] Scraping MedmonkMilpitas, CA, USA...\n","[235/500] Scraping ShoobsLondon, England, United Kingdom...\n","[236/500] Scraping PushbulletSan Francisco, CA, USA...\n","[237/500] Scraping PovioSan Francisco, CA, USA...\n","[238/500] Scraping PiinPointKitchener, ON, Canada...\n","[239/500] Scraping One DegreeSan Francisco, CA, USA...\n","[240/500] Scraping MBXSan Francisco, CA, USA...\n","[241/500] Scraping EmailioRemote...\n","[242/500] Scraping DevCycleToronto, ON, Canada...\n","[243/500] Scraping CodeNowChicago, IL, USA...\n","[244/500] Scraping CodeCombatSan Francisco, CA, USA...\n","[245/500] Scraping Noora HealthBengaluru, KA, India...\n","[246/500] Scraping CamblySan Francisco, CA, USA...\n","[247/500] Scraping AptDecoNew York, NY, USA...\n","[248/500] Scraping CareMessageSan Francisco, CA, USA...\n","[249/500] Scraping AmbitionChattanooga, TN, USA...\n","[250/500] Scraping AirHelpRemote...\n","[251/500] Scraping 42San Francisco, CA, USA...\n","[252/500] Scraping True LinkSan Francisco, CA, USA...\n","[253/500] Scraping ReebeeKitchener, ON, Canada...\n","[254/500] Scraping LobSan Francisco, CA, USA...\n","[255/500] Scraping Panorama EducationBoston, MA, USA...\n","[256/500] Scraping CratejoyAustin, TX, USA...\n","[257/500] Scraping Estimote, Inc.Kraków, Lesser Poland Voivodeship, Poland...\n","[258/500] Scraping EasyPostSan Francisco, CA, USA...\n","[259/500] Scraping AssetaNewport Beach, CA, USA...\n","[260/500] Scraping Per VicesToronto, ON, Canada...\n","[261/500] Scraping 7cupsVirginia Beach, VA, USA...\n","[262/500] Scraping WatsiSan Francisco, CA, USA...\n","[263/500] Scraping GoldbellyNew York, NY, USA...\n","[264/500] Scraping WevorceBothell, WA, USA...\n","[265/500] Scraping WefunderSan Francisco, CA, USA...\n","[266/500] Scraping SimplyInsuredSan Francisco, CA, USA...\n","[267/500] Scraping CircuitHubLondon, England, United Kingdom...\n","[268/500] Scraping ExperimentHonolulu, HI, USA...\n","[269/500] Scraping RADARNew York, NY, USA...\n","[270/500] Scraping LollipuffMountain View, CA, USA...\n","[271/500] Scraping StrikinglyShanghai, Shanghai, China...\n","[272/500] Scraping InfluxDataSan Francisco, CA, USA...\n","[273/500] Scraping EtleapSan Francisco, CA, USA...\n","[274/500] Scraping CheetahSan Francisco, CA, USA...\n","[275/500] Scraping LawdingoNew York, NY, USA...\n","[276/500] Scraping PadletSan Francisco, CA, USA...\n","[277/500] Scraping BuildZoomSan Francisco, CA, USA...\n","[278/500] Scraping CircuitLabRedwood City, CA, USA...\n","[279/500] Scraping StyleUpNew York, NY, USA...\n","[280/500] Scraping 9gagHong Kong, Hong Kong...\n","[281/500] Scraping SvbtleSan Francisco, CA, USA...\n","[282/500] Scraping PlivoAustin, TX, USA...\n","[283/500] Scraping Double RoboticsBurlingame, CA, USA...\n","[284/500] Scraping VastrmBurlingame, CA, USA...\n","[285/500] Scraping FlightfoxBoulder, CO, USA...\n","[286/500] Scraping BackerKitSan Francisco, CA, USA...\n","[287/500] Scraping SubmittableMissoula, MT, USA...\n","[288/500] Scraping NewsBlurSan Francisco, CA, USA...\n","[289/500] Scraping OctaveWealthSan Francisco, CA, USA...\n","[290/500] Scraping UpwaveSan Francisco, CA, USA...\n","[291/500] Scraping EligibleNY, USA...\n","[292/500] Scraping FundersClubSan Francisco, CA, USA...\n","[293/500] Scraping ZentailColumbia, MD, USA...\n","[294/500] Scraping MattermostPalo Alto, CA, USA...\n","[295/500] Scraping HealthSherpaSacramento, CA, USA...\n","[296/500] Scraping Human DxSan Francisco, CA, USA...\n","[297/500] Scraping Mth SenseSan Jose, CA, USA...\n","[298/500] Scraping RainforestSan Francisco, CA, USA...\n","[299/500] Scraping ScreenleapSan Carlos, CA, USA...\n","[300/500] Scraping BuxferSanta Clara, CA, USA...\n","[301/500] Scraping VirtualminMountain View, CA, USA...\n","[302/500] Scraping Blue Frog GamingAkron, OH, USA...\n","[303/500] Scraping CleverDeckIstanbul, Istanbul, Turkey...\n","[304/500] Scraping MartiniSan Francisco, CA, USA...\n","[305/500] Scraping ShofoSan Francisco, CA, USA...\n","[306/500] Scraping GrazeMateSan Francisco, CA, USA...\n","[307/500] Scraping RitivelSan Francisco, CA, USA...\n","[308/500] Scraping SpotPaySan Francisco, CA, USA...\n","[309/500] Scraping VeriadSan Francisco, CA, USA...\n","[310/500] Scraping Coevolved...\n","[311/500] Scraping Fed10San Francisco, CA, USA...\n","[312/500] Scraping Squid...\n","[313/500] Scraping Scout OutSan Francisco, CA, USA...\n","[314/500] Scraping CorelayerSan Francisco, CA, USA...\n","[315/500] Scraping ChasiNew York, NY, USA...\n","[316/500] Scraping o11San Francisco, CA, USA...\n","[317/500] Scraping Sequence MarketsNew York, NY, USA...\n","[318/500] Scraping ArzuleSan Francisco, CA, USA...\n","[319/500] Scraping Voxel Energy...\n","[320/500] Scraping Bubble Lab...\n","[321/500] Scraping PocketSan Francisco, CA, USA...\n","[322/500] Scraping ByteportSan Francisco, CA, USA...\n","[323/500] Scraping Constellation SpaceSeattle, WA, USA...\n","[324/500] Scraping AxionOrbital SpaceSan Francisco, CA, USA...\n","[325/500] Scraping CarettaSan Francisco, CA, USA...\n","[326/500] Scraping Human ArchiveSan Francisco, CA, USA...\n","[327/500] Scraping Ressl AISan Francisco, CA, USA...\n","[328/500] Scraping SideKitSan Francisco, CA, USA...\n","[329/500] Scraping Clice AISan Francisco, CA, USA...\n","[330/500] Scraping ChamberSan Francisco, CA, USA...\n","[331/500] Scraping Patientdesk.ai...\n","[332/500] Scraping Visibl SemiconductorsSan Francisco, CA, USA...\n","[333/500] Scraping Seeing SystemsLondon, England, United Kingdom...\n","[334/500] Scraping CardboardSan Francisco, CA, USA...\n","[335/500] Scraping PranaSan Francisco, CA, USA...\n","[336/500] Scraping Pax HistoriaSan Francisco, CA, USA...\n","[337/500] Scraping Laurence...\n","[338/500] Scraping Opalite HealthPalo Alto, CA, USA...\n","[339/500] Scraping Q2QSan Francisco, CA, USA...\n","[340/500] Scraping PerfectlySan Francisco, CA, USA...\n","[341/500] Scraping Ishiki Labs...\n","[342/500] Scraping SparklesSan Francisco, CA, USA...\n","[343/500] Scraping ArclineSan Francisco, CA, USA...\n","[344/500] Scraping RamAInSan Francisco, CA, USA...\n","[345/500] Scraping OctaPulseRemote...\n","[346/500] Scraping LuelSan Francisco, CA, USA...\n","[347/500] Scraping SkillsyncSan Francisco, CA, USA...\n","[348/500] Scraping AutoSitu...\n","[349/500] Scraping VerdexSan Francisco, CA, USA...\n","[350/500] Scraping Cumulus Labs...\n","[351/500] Scraping 21stSan Francisco, CA, USA...\n","[352/500] Scraping AvoiceSan Francisco, CA, USA...\n","[353/500] Scraping Bujo AISan Francisco, CA, USA...\n","[354/500] Scraping BurtSan Francisco, CA, USA...\n","[355/500] Scraping BaseFrameSan Francisco, CA, USA...\n","[356/500] Scraping ProtentSan Francisco, CA, USA...\n","[357/500] Scraping LegalOSSan Francisco, CA, USA...\n","[358/500] Scraping 10x ScienceSan Francisco, CA, USA...\n","[359/500] Scraping LexiusSan Francisco, CA, USA...\n","[360/500] Scraping SonarlySan Francisco, CA, USA...\n","[361/500] Scraping RunAnywhereSan Francisco, CA, USA...\n","[362/500] Scraping KitaSan Francisco, CA, USA...\n","[363/500] Scraping DiditSan Francisco, CA, USA...\n","[364/500] Scraping TensolSan Francisco, CA, USA...\n","[365/500] Scraping PantaSan Francisco, CA, USA...\n","[366/500] Scraping HlabsAustin, TX, USA...\n","[367/500] Scraping ValgoSan Mateo, CA, USA...\n","[368/500] Scraping Corvera...\n","[369/500] Scraping Mantis BiotechnologyNew York, NY, USA...\n","[370/500] Scraping HaladirSan Francisco, CA, USA...\n","[371/500] Scraping JSX Tool...\n","[372/500] Scraping Zephyr FusionSan Diego, CA, USA...\n","[373/500] Scraping MayflowerSan Francisco, CA, USA...\n","[374/500] Scraping itemSan Francisco, CA, USA...\n","[375/500] Scraping RivetSan Francisco, CA, USA...\n","[376/500] Scraping OpenrollStockholm, Stockholm County, Sweden...\n","[377/500] Scraping Fixpoint...\n","[378/500] Scraping BearSan Francisco, CA, USA...\n","[379/500] Scraping MossSan Mateo, CA, USA...\n","[380/500] Scraping ZarnaSan Francisco, CA, USA...\n","[381/500] Scraping MarkItSan Francisco, CA, USA...\n","[382/500] Scraping Dome...\n","[383/500] Scraping VoltairSan Francisco, CA, USA...\n","[384/500] Scraping Clicks...\n","[385/500] Scraping Fastshot...\n","[386/500] Scraping ShoptiquesNew York, NY, USA...\n","[387/500] Scraping ClaybirdSan Francisco, CA, USA...\n","[388/500] Scraping LunaBillSan Francisco, CA, USA...\n","[389/500] Scraping Axial Composites Industries...\n","[390/500] Scraping VelvetSan Francisco, CA, USA...\n","[391/500] Scraping DiligenceSquaredSan Francisco, CA, USA...\n","[392/500] Scraping Amera...\n","[393/500] Scraping Forge RoboticsSan Francisco, CA, USA...\n","[394/500] Scraping Stratus AviationSan Francisco, CA, USA...\n","[395/500] Scraping BlumaSan Francisco, CA, USA...\n","[396/500] Scraping LightberrySan Francisco, CA, USA...\n","[397/500] Scraping The Hog...\n","[398/500] Scraping Telemetron...\n","[399/500] Scraping Boom AISan Francisco, CA, USA...\n","[400/500] Scraping Freeport MarketsNew York, NY, USA...\n","[401/500] Scraping MiniswapSan Francisco, CA, USA...\n","[402/500] Scraping Rovi HealthNew York, NY, USA...\n","[403/500] Scraping Castari...\n","[404/500] Scraping JarminSan Francisco, CA, USA...\n","[405/500] Scraping ExonicSan Francisco, CA, USA...\n","[406/500] Scraping LemmaSan Francisco, CA, USA...\n","[407/500] Scraping Mantle...\n","[408/500] Scraping Karumi...\n","[409/500] Scraping IcarusLos Angeles, CA, USA...\n","[410/500] Scraping Sava...\n","[411/500] Scraping Waffer...\n","[412/500] Scraping DenkiSan Francisco, CA, USA...\n","[413/500] Scraping ArctenSan Francisco, CA, USA...\n","[414/500] Scraping MetorialRemote...\n","[415/500] Scraping Kalpa Labs...\n","[416/500] Scraping Absurd...\n","[417/500] Scraping Clad LabsSan Francisco, CA, USA...\n","[418/500] Scraping Automax.aiSan Francisco, CA, USA...\n","[419/500] Scraping SpecificStockholm, Stockholm County, Sweden...\n","[420/500] Scraping PrimerSan Francisco, CA, USA...\n","[421/500] Scraping NarrativeSan Francisco, CA, USA...\n","[422/500] Scraping Crunched...\n","[423/500] Scraping Lua Global Inc...\n","[424/500] Scraping Spatial AIPalo Alto, CA, USA...\n","[425/500] Scraping SourcebotSan Francisco, CA, USA...\n","[426/500] Scraping ZalosSan Francisco, CA, USA...\n","[427/500] Scraping HireGlideSan Francisco, CA, USA...\n","[428/500] Scraping AsideSan Francisco, CA, USA...\n","[429/500] Scraping Scott AINew York, NY, USA...\n","[430/500] Scraping EverestSan Francisco, CA, USA...\n","[431/500] Scraping Expected ParrotCambridge, MA, USA...\n","[432/500] Scraping Imagine AISan Francisco, CA, USA...\n","[433/500] Scraping Anto BiosciencesSan Francisco, CA, USA...\n","[434/500] Scraping Patent WatchToronto, ON, Canada...\n","[435/500] Scraping s2.devSan Francisco, CA, USA...\n","[436/500] Scraping Tornyol...\n","[437/500] Scraping NucleoSan Francisco, CA, USA...\n","[438/500] Scraping LeadbaySan Francisco, CA, USA...\n","[439/500] Scraping Pixley AISan Francisco, CA, USA...\n","[440/500] Scraping Cortex AISan Francisco, CA, USA...\n","[441/500] Scraping InspectorSan Francisco, CA, USA...\n","[442/500] Scraping ThesisSan Francisco, CA, USA...\n","[443/500] Scraping OxusSan Francisco, CA, USA...\n","[444/500] Scraping Play HealthBoulder, CO, USA...\n","[445/500] Scraping CaseySan Francisco, CA, USA...\n","[446/500] Scraping SelfinSan Francisco, CA, USA...\n","[447/500] Scraping CompyleSan Francisco, CA, USA...\n","[448/500] Scraping LexiSan Francisco, CA, USA...\n","[449/500] Scraping Codyco...\n","[450/500] Scraping ComplyDo...\n","[451/500] Scraping BraviSan Francisco, CA, USA...\n","[452/500] Scraping RovrSan Francisco, CA, USA...\n","[453/500] Scraping Sunflower...\n","[454/500] Scraping CaptainSan Francisco, CA, USA...\n","[455/500] Scraping RedaptoSan Francisco, CA, USA...\n","[456/500] Scraping ArticulateSan Francisco, CA, USA...\n","[457/500] Scraping Aleph LabSan Francisco, CA, USA...\n","[458/500] Scraping LogicalSan Francisco, CA, USA...\n","[459/500] Scraping Agentic FabriqSunnyvale, CA, USA...\n","[460/500] Scraping TravoSan Francisco, CA, USA...\n","[461/500] Scraping TensrSan Francisco, CA, USA...\n","[462/500] Scraping Kestrel AISan Francisco, CA, USA...\n","[463/500] Scraping SanctumSan Francisco, CA, USA...\n","[464/500] Scraping LogosGuardSan Francisco, CA, USA...\n","[465/500] Scraping SciloopSan Francisco, CA, USA...\n","[466/500] Scraping MultifactorSan Francisco, CA, USA...\n","[467/500] Scraping Allus AIAtlanta, GA, USA...\n","[468/500] Scraping Unsiloed AISan Francisco, CA, USA...\n","[469/500] Scraping SF Tensor...\n","[470/500] Scraping AnswerThis...\n","[471/500] Scraping NivaraSan Francisco, CA, USA...\n","[472/500] Scraping LakoniaSan Francisco, CA, USA...\n","[473/500] Scraping SellRazeSan Francisco, CA, USA...\n","[474/500] Scraping Mod AI...\n","[475/500] Scraping Semble AISan Francisco, CA, USA...\n","[476/500] Scraping Flick...\n","[477/500] Scraping AtlasGridMountain View, CA, USA...\n","[478/500] Scraping Wardstone...\n","[479/500] Scraping NessieSan Francisco, CA, USA...\n","[480/500] Scraping Hypercubic...\n","[481/500] Scraping Cranston AISan Francisco, CA, USA...\n","[482/500] Scraping QuestomSan Francisco, CA, USA...\n","[483/500] Scraping Velum LabsSan Francisco, CA, USA...\n","[484/500] Scraping Veria LabsSan Francisco, CA, USA...\n","[485/500] Scraping IrisSan Francisco, CA, USA...\n","[486/500] Scraping PlayVisionSan Francisco, CA, USA...\n","[487/500] Scraping KoyalSan Francisco, CA, USA...\n","[488/500] Scraping Kaigo HealthSan Francisco, CA, USA...\n","[489/500] Scraping NorraSan Francisco, CA, USA...\n","[490/500] Scraping LunavoSan Francisco, CA, USA...\n","[491/500] Scraping DariSan Francisco, CA, USA...\n","[492/500] Scraping The Context CompanySan Francisco, CA, USA...\n","[493/500] Scraping SorceSan Francisco, CA, USA...\n","[494/500] Scraping Caddy...\n","[495/500] Scraping Alt-XSan Francisco, CA, USA...\n","[496/500] Scraping Locus...\n","[497/500] Scraping DeeptraceSan Francisco, CA, USA...\n","[498/500] Scraping Aspect...\n","[499/500] Scraping ScoopSan Francisco, CA, USA...\n","[500/500] Scraping HyperspellSan Francisco, CA, USA...\n","Done! Data saved to yc_startups_500.csv\n"]}]}]}